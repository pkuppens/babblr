# Docker Environment Configuration Template
# Copy this file to .env and configure with your values
# Usage: cp .env.docker .env

# ============================================
# LLM Provider Configuration
# ============================================
# Options: ollama, claude, gemini, mock
LLM_PROVIDER=ollama

# Ollama Configuration (for local LLM)
# Options: llama3.2:1b (fast, dev), llama3.2:latest (3B, balanced), llama3.1:8b (better quality)
# Recommendation: Use 1b during development, 3b+ for production
OLLAMA_MODEL=llama3.2:1b

# Claude API Configuration (if using LLM_PROVIDER=claude)
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Gemini API Configuration (if using LLM_PROVIDER=gemini)
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=

# ============================================
# Whisper STT Configuration
# ============================================
# Options: tiny, base, small, medium, large, large-v2, large-v3, turbo
# Larger models are more accurate but slower and use more memory
# Default: large-v3 (best accuracy for non-native speakers)
WHISPER_MODEL=large-v3

# Device selection for local Whisper in the backend (auto, cuda, cpu)
WHISPER_DEVICE=auto

# STT provider (local or whisper_webservice)
STT_PROVIDER=local
STT_WEBSERVICE_URL=http://babblr-whisper:9000
STT_WEBSERVICE_TIMEOUT=300
STT_WEBSERVICE_DEVICE=auto

# Whisper container settings (used by docker-compose.whisper.yml)
WHISPER_CONTAINER_MODEL=large-v3
WHISPER_CONTAINER_ENGINE=openai_whisper
WHISPER_CONTAINER_DEVICE=cpu

# ============================================
# Performance Configuration
# ============================================
# Text correction maximum level ("0" = no corrections, "A1", "A2", "B1", "B2", "C1", "C2")
# Corrections add 500-1000ms latency per message
# Default: A2 (correct A1 and A2 learners, skip B1+)
CORRECTION_MAX_LEVEL=A2

# Maximum conversation history messages to include in LLM context
# Lower = faster responses, higher = better context
# Default: 5 messages (last 5 exchanges)
CONVERSATION_MAX_HISTORY=5

# ============================================
# Database Configuration (for production)
# ============================================
# Development uses PostgreSQL in Docker with these defaults:
# - User: babblr
# - Password: devpassword
# - Database: babblr
# - Host: postgres (Docker service name)
# - Port: 5432
#
# For production, override BABBLR_CONVERSATION_DATABASE_URL:
# BABBLR_CONVERSATION_DATABASE_URL=postgresql+asyncpg://user:pass@host:5432/dbname
