{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "This notebook demonstrates loading raw Parquet files into Delta Lake tables.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Delta Lake table creation\n",
    "- Schema inference from Parquet\n",
    "- DBFS file access\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:**\n",
    "- Generated parquet files in `databricks/data/` (run `python generate_synthetic_data.py` locally)\n",
    "- Uploaded files to **Unity Catalog Volumes** (required for Spark access in Free Edition):\n",
    "  - Create catalog (typically `workspace` in Free Edition) → Schema `babblr` → Volume `bronze`\n",
    "  - Upload all 7 parquet files locally to `/Volumes/<catalog>/babblr/bronze/`\n",
    "  - **Note**: This tutorial uses local file uploads (not external AWS/S3 volumes)\n",
    "  \n",
    "> **Important**: Workspace folders cannot be accessed directly by Spark for data files. You must use Unity Catalog Volumes for medallion architecture. See the validation guide (section 2.2) for step-by-step instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Define paths and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# IMPORTANT: Unity Catalog Volumes are required for Spark to access data files.\n",
    "# Workspace folders cannot be accessed directly by Spark for data files.\n",
    "# For Free Edition: Use local file uploads to Unity Catalog Volumes (not external AWS/S3 volumes).\n",
    "\n",
    "# Common catalog names to try (Free Edition typically uses 'workspace')\n",
    "# Replace with your catalog name if different\n",
    "COMMON_CATALOGS = [\"workspace\", \"main\", \"hive_metastore\"]\n",
    "\n",
    "# Build list of possible Volume paths\n",
    "POSSIBLE_PATHS = []\n",
    "for catalog in COMMON_CATALOGS:\n",
    "    POSSIBLE_PATHS.append(f\"/Volumes/{catalog}/babblr/bronze\")\n",
    "\n",
    "# Also try DBFS FileStore (may be restricted in Free Edition)\n",
    "POSSIBLE_PATHS.append(\"/FileStore/babblr/bronze\")\n",
    "\n",
    "DATABASE_NAME = \"babblr_bronze\"\n",
    "\n",
    "# Detect which path is available\n",
    "BRONZE_PATH = None\n",
    "for path in POSSIBLE_PATHS:\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        if files:  # Check that directory exists and has files\n",
    "            BRONZE_PATH = path\n",
    "            print(f\"[OK] Found accessible path: {BRONZE_PATH}\")\n",
    "            print(f\"     Found {len(files)} file(s) in directory\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "if BRONZE_PATH is None:\n",
    "    print(\"[ERROR] No accessible storage path found!\")\n",
    "    print(\"\\n[SOLUTION] You must use Unity Catalog Volumes (local file uploads in Free Edition):\")\n",
    "    print(\"   1. Go to Catalog in sidebar\")\n",
    "    print(\"   2. Create or select a catalog (typically 'workspace' in Free Edition)\")\n",
    "    print(\"   3. Create schema 'babblr'\")\n",
    "    print(\"   4. Create volume 'bronze'\")\n",
    "    print(\"   5. Upload all 7 parquet files locally to the volume (not external AWS/S3)\")\n",
    "    print(\"   6. Files will be at: /Volumes/<catalog>/babblr/bronze/\")\n",
    "    print(\"\\n   Workspace folders do NOT work for Spark data access.\")\n",
    "    print(\"   See VALIDATION.md section 2.2 for detailed steps.\")\n",
    "    raise FileNotFoundError(\"No accessible data path found. Please use Unity Catalog Volumes.\")\n",
    "\n",
    "# Create database if not exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "print(f\"Using database: {DATABASE_NAME}\")\n",
    "print(f\"Using bronze path: {BRONZE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files are available\n",
    "try:\n",
    "    files = dbutils.fs.ls(BRONZE_PATH)\n",
    "    print(\"Available files in bronze layer:\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f.name} ({f.size / 1024:.1f} KB)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nPlease upload Parquet files to {BRONZE_PATH}\")\n",
    "    print(\"Run generate_synthetic_data.py locally first, then upload the data/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tables into Delta format\n",
    "\n",
    "Delta Lake provides:\n",
    "- ACID transactions\n",
    "- Time travel (versioning)\n",
    "- Schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_delta(table_name: str):\n",
    "    \"\"\"Load a Parquet file into a Delta table.\"\"\"\n",
    "    parquet_path = f\"{BRONZE_PATH}/{table_name}.parquet\"\n",
    "\n",
    "    try:\n",
    "        # Read Parquet with schema inference\n",
    "        df = spark.read.parquet(parquet_path)\n",
    "\n",
    "        # Write as Delta table (overwrite for demo purposes)\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "        row_count = spark.table(table_name).count()\n",
    "        print(f\"[OK] {table_name}: {row_count} rows loaded\")\n",
    "        return row_count\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] {table_name}: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables\n",
    "tables = [\n",
    "    \"conversations\",\n",
    "    \"messages\",\n",
    "    \"lessons\",\n",
    "    \"lesson_progress\",\n",
    "    \"assessments\",\n",
    "    \"assessment_attempts\",\n",
    "    \"user_levels\"\n",
    "]\n",
    "\n",
    "total_rows = 0\n",
    "for table in tables:\n",
    "    total_rows += load_to_delta(table)\n",
    "\n",
    "print(f\"\\nTotal: {total_rows} rows loaded into Delta tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Show all tables in the bronze database\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Quick preview of conversations table\n",
    "SELECT * FROM conversations LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Check data distribution by language\n",
    "SELECT\n",
    "    language,\n",
    "    COUNT(*) as conversation_count,\n",
    "    COUNT(DISTINCT user_id) as unique_users\n",
    "FROM conversations\n",
    "GROUP BY language\n",
    "ORDER BY conversation_count DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Feature: Table History\n",
    "\n",
    "Delta Lake automatically tracks all changes to tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- View table history (time travel metadata)\n",
    "DESCRIBE HISTORY conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Feature: Schema Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- View schema of a table\n",
    "DESCRIBE TABLE EXTENDED conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Created a Bronze database for raw data\n",
    "2. Loaded Parquet files into Delta Lake tables\n",
    "3. Verified data with basic queries\n",
    "4. Demonstrated Delta Lake features (history, schema)\n",
    "\n",
    "**Next:** Run `02_silver_layer` to clean and transform the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
