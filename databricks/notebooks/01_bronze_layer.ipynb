{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "This notebook demonstrates loading raw Parquet files into Delta Lake tables.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Delta Lake table creation\n",
    "- Schema inference from Parquet\n",
    "- DBFS file access\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:**\n",
    "- Generated parquet files in `databricks/data/` (run `python generate_synthetic_data.py`)\n",
    "- Uploaded files to Databricks storage (DBFS, Volumes, or workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Define paths and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# For Free Edition: Use Volumes or upload files directly to workspace\n",
    "# Try these paths in order (first available will be used):\n",
    "POSSIBLE_PATHS = [\n",
    "    \"/Volumes/babblr/bronze\",  # Volumes (Free Edition compatible)\n",
    "    \"/FileStore/babblr/bronze\",  # DBFS FileStore (may be restricted in Free Edition)\n",
    "    \"/Workspace/babblr/bronze\",  # Workspace files (alternative)\n",
    "]\n",
    "\n",
    "DATABASE_NAME = \"babblr_bronze\"\n",
    "\n",
    "# Detect which path is available\n",
    "BRONZE_PATH = None\n",
    "for path in POSSIBLE_PATHS:\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        BRONZE_PATH = path\n",
    "        print(f\"[OK] Found accessible path: {BRONZE_PATH}\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if BRONZE_PATH is None:\n",
    "    print(\"[WARNING] No accessible storage path found. For Free Edition:\")\n",
    "    print(\"   1. Upload files using 'Upload Data' in the workspace\")\n",
    "    print(\"   2. Or use Volumes: Create a Volume at /Volumes/babblr/bronze\")\n",
    "    print(\"   3. Or upload files directly in this notebook using:\")\n",
    "    print(\"      dbutils.fs.put('/tmp/your_file.parquet', file_content)\")\n",
    "    print(\"\\n   Then update BRONZE_PATH above to match your upload location.\")\n",
    "    BRONZE_PATH = \"/tmp/babblr/bronze\"  # Fallback to temp location\n",
    "    print(f\"\\n   Using fallback path: {BRONZE_PATH}\")\n",
    "\n",
    "# Create database if not exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "print(f\"Using database: {DATABASE_NAME}\")\n",
    "print(f\"Using bronze path: {BRONZE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files are available\n",
    "try:\n",
    "    files = dbutils.fs.ls(BRONZE_PATH)\n",
    "    print(\"Available files in bronze layer:\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f.name} ({f.size / 1024:.1f} KB)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nPlease upload Parquet files to {BRONZE_PATH}\")\n",
    "    print(\"Run generate_synthetic_data.py locally first, then upload the data/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tables into Delta format\n",
    "\n",
    "Delta Lake provides:\n",
    "- ACID transactions\n",
    "- Time travel (versioning)\n",
    "- Schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_delta(table_name: str):\n",
    "    \"\"\"Load a Parquet file into a Delta table.\"\"\"\n",
    "    parquet_path = f\"{BRONZE_PATH}/{table_name}.parquet\"\n",
    "\n",
    "    try:\n",
    "        # Read Parquet with schema inference\n",
    "        df = spark.read.parquet(parquet_path)\n",
    "\n",
    "        # Write as Delta table (overwrite for demo purposes)\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "        row_count = spark.table(table_name).count()\n",
    "        print(f\"[OK] {table_name}: {row_count} rows loaded\")\n",
    "        return row_count\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] {table_name}: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables\n",
    "tables = [\n",
    "    \"conversations\",\n",
    "    \"messages\",\n",
    "    \"lessons\",\n",
    "    \"lesson_progress\",\n",
    "    \"assessments\",\n",
    "    \"assessment_attempts\",\n",
    "    \"user_levels\"\n",
    "]\n",
    "\n",
    "total_rows = 0\n",
    "for table in tables:\n",
    "    total_rows += load_to_delta(table)\n",
    "\n",
    "print(f\"\\nTotal: {total_rows} rows loaded into Delta tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Show all tables in the bronze database\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Quick preview of conversations table\n",
    "SELECT * FROM conversations LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Check data distribution by language\n",
    "SELECT\n",
    "    language,\n",
    "    COUNT(*) as conversation_count,\n",
    "    COUNT(DISTINCT user_id) as unique_users\n",
    "FROM conversations\n",
    "GROUP BY language\n",
    "ORDER BY conversation_count DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Feature: Table History\n",
    "\n",
    "Delta Lake automatically tracks all changes to tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- View table history (time travel metadata)\n",
    "DESCRIBE HISTORY conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Feature: Schema Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- View schema of a table\n",
    "DESCRIBE TABLE EXTENDED conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Created a Bronze database for raw data\n",
    "2. Loaded Parquet files into Delta Lake tables\n",
    "3. Verified data with basic queries\n",
    "4. Demonstrated Delta Lake features (history, schema)\n",
    "\n",
    "**Next:** Run `02_silver_layer` to clean and transform the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
