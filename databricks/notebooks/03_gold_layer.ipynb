{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Analytics & Clustering\n",
    "\n",
    "This notebook creates business-ready aggregates and demonstrates K-Means clustering for student segmentation.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Gold layer aggregations\n",
    "- Complex SQL analytics\n",
    "- K-Means clustering for student segmentation\n",
    "- MLflow experiment tracking (optional, not available in Free Edition)\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** Run `02_silver_layer` first to create the silver tables.\n",
    "\n",
    "> **Note**: MLflow is not available in Databricks Free Edition. The clustering model training works without MLflow tracking. If you're using a paid edition, MLflow tracking will be enabled automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type stub for Databricks runtime variable\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from pyspark.sql import SparkSession  # pyright: ignore[reportMissingImports]\n",
    "    spark: SparkSession  # Provided by Databricks runtime\n",
    "\n",
    "from pyspark.sql import functions as F  # type: ignore[reportMissingImports]  # Databricks-only module\n",
    "from pyspark.sql.window import Window  # type: ignore[reportMissingImports]  # Databricks-only module\n",
    "\n",
    "# Try to import MLflow (not available in Free Edition)\n",
    "try:\n",
    "    import mlflow  # type: ignore[reportMissingImports]  # Databricks-only module\n",
    "    import mlflow.spark  # type: ignore[reportMissingImports]  # Databricks-only module\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"[INFO] MLflow not available (Free Edition). Clustering will work without MLflow tracking.\")\n",
    "\n",
    "# Configuration\n",
    "SILVER_DB = \"babblr_silver\"\n",
    "GOLD_DB = \"babblr_gold\"\n",
    "\n",
    "# Create gold database\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {GOLD_DB}\")  # Provided by Databricks runtime\n",
    "print(f\"Gold database: {GOLD_DB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daily Learning Metrics\n",
    "\n",
    "Aggregate daily metrics for dashboard KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE babblr_gold.daily_metrics AS\n",
    "SELECT\n",
    "    DATE(created_at) as activity_date,\n",
    "    language,\n",
    "    COUNT(DISTINCT user_id) as active_users,\n",
    "    COUNT(*) as conversations,\n",
    "    SUM(message_count) as total_messages,\n",
    "    ROUND(AVG(duration_minutes), 1) as avg_session_minutes,\n",
    "    ROUND(AVG(error_rate), 3) as avg_error_rate\n",
    "FROM babblr_silver.conversations\n",
    "GROUP BY DATE(created_at), language\n",
    "ORDER BY activity_date, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Preview daily metrics\n",
    "SELECT * FROM babblr_gold.daily_metrics\n",
    "ORDER BY activity_date DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lesson Effectiveness Scores\n",
    "\n",
    "Calculate which lessons lead to the best learning outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE babblr_gold.lesson_effectiveness AS\n",
    "WITH lesson_stats AS (\n",
    "    SELECT\n",
    "        lesson_id,\n",
    "        lesson_type,\n",
    "        subject,\n",
    "        lesson_difficulty,\n",
    "        COUNT(*) as total_attempts,\n",
    "        SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completions,\n",
    "        AVG(CASE WHEN status = 'completed' THEN mastery_score END) as avg_mastery,\n",
    "        AVG(time_to_complete_hours) as avg_completion_time\n",
    "    FROM babblr_silver.lesson_progress\n",
    "    GROUP BY lesson_id, lesson_type, subject, lesson_difficulty\n",
    ")\n",
    "SELECT\n",
    "    *,\n",
    "    ROUND(completions * 1.0 / total_attempts, 3) as completion_rate,\n",
    "    -- Effectiveness score: weighted combination of completion rate and mastery\n",
    "    ROUND(\n",
    "        (completions * 1.0 / total_attempts) * 0.4 +\n",
    "        COALESCE(avg_mastery, 0) * 0.6,\n",
    "        3\n",
    "    ) as effectiveness_score\n",
    "FROM lesson_stats\n",
    "WHERE total_attempts >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Top 10 most effective lessons\n",
    "SELECT\n",
    "    lesson_type,\n",
    "    subject,\n",
    "    lesson_difficulty,\n",
    "    total_attempts,\n",
    "    completion_rate,\n",
    "    ROUND(avg_mastery, 3) as avg_mastery,\n",
    "    effectiveness_score\n",
    "FROM babblr_gold.lesson_effectiveness\n",
    "ORDER BY effectiveness_score DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CEFR Level Progression Funnel\n",
    "\n",
    "Track how students progress through CEFR levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE babblr_gold.cefr_funnel AS\n",
    "SELECT\n",
    "    language,\n",
    "    recommended_level as cefr_level,\n",
    "    COUNT(DISTINCT user_id) as users_at_level,\n",
    "    ROUND(AVG(score), 1) as avg_assessment_score,\n",
    "    COUNT(*) as total_assessments\n",
    "FROM babblr_silver.assessment_attempts\n",
    "GROUP BY language, recommended_level\n",
    "ORDER BY language,\n",
    "    CASE recommended_level\n",
    "        WHEN 'A1' THEN 1\n",
    "        WHEN 'A2' THEN 2\n",
    "        WHEN 'B1' THEN 3\n",
    "        WHEN 'B2' THEN 4\n",
    "        WHEN 'C1' THEN 5\n",
    "        WHEN 'C2' THEN 6\n",
    "    END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- CEFR funnel visualization data\n",
    "SELECT * FROM babblr_gold.cefr_funnel\n",
    "WHERE language = 'spanish'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Engagement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE babblr_gold.topic_engagement AS\n",
    "SELECT\n",
    "    topic_id,\n",
    "    language,\n",
    "    COUNT(*) as conversation_count,\n",
    "    COUNT(DISTINCT user_id) as unique_users,\n",
    "    ROUND(AVG(message_count), 1) as avg_messages_per_conv,\n",
    "    ROUND(AVG(duration_minutes), 1) as avg_duration_min,\n",
    "    ROUND(AVG(error_rate), 3) as avg_error_rate\n",
    "FROM babblr_silver.conversations\n",
    "WHERE topic_id IS NOT NULL\n",
    "GROUP BY topic_id, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Most engaging topics (by session duration)\n",
    "SELECT\n",
    "    topic_id,\n",
    "    SUM(conversation_count) as total_conversations,\n",
    "    ROUND(AVG(avg_duration_min), 1) as avg_duration,\n",
    "    ROUND(AVG(avg_messages_per_conv), 1) as avg_messages\n",
    "FROM babblr_gold.topic_engagement\n",
    "GROUP BY topic_id\n",
    "ORDER BY avg_duration DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Student Clustering\n",
    "\n",
    "Demonstrate K-Means clustering to segment students by learning patterns. MLflow tracking is optional (not available in Free Edition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler  # type: ignore[reportMissingImports]  # Databricks-only module\n",
    "from pyspark.ml.clustering import KMeans  # type: ignore[reportMissingImports]  # Databricks-only module\n",
    "from pyspark.ml import Pipeline  # type: ignore[reportMissingImports]  # Databricks-only module\n",
    "\n",
    "# Prepare features for clustering\n",
    "user_features = spark.sql(\"\"\"  # type: ignore[reportUndefinedVariable]  # Provided by Databricks runtime\n",
    "    SELECT\n",
    "        up.user_id,\n",
    "        up.total_assessments,\n",
    "        up.avg_score,\n",
    "        COALESCE(up.avg_improvement, 0) as avg_improvement,\n",
    "        up.proficiency_score,\n",
    "        COALESCE(conv.total_conversations, 0) as total_conversations,\n",
    "        COALESCE(conv.avg_error_rate, 0) as avg_error_rate,\n",
    "        COALESCE(lp.completed_lessons, 0) as completed_lessons,\n",
    "        COALESCE(lp.avg_mastery, 0) as avg_mastery\n",
    "    FROM babblr_silver.user_profiles up\n",
    "    LEFT JOIN (\n",
    "        SELECT user_id, COUNT(*) as total_conversations, AVG(error_rate) as avg_error_rate\n",
    "        FROM babblr_silver.conversations\n",
    "        GROUP BY user_id\n",
    "    ) conv ON up.user_id = conv.user_id\n",
    "    LEFT JOIN (\n",
    "        SELECT user_id, COUNT(*) as completed_lessons, AVG(mastery_score) as avg_mastery\n",
    "        FROM babblr_silver.lesson_progress\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY user_id\n",
    "    ) lp ON up.user_id = lp.user_id\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Users for clustering: {user_features.count()}\")\n",
    "user_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Experiment Setup (Optional - Not Available in Free Edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name (only if MLflow is available)\n",
    "if MLFLOW_AVAILABLE:\n",
    "    experiment_name = \"/Shared/babblr_student_segmentation\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"MLflow experiment: {experiment_name}\")\n",
    "else:\n",
    "    print(\"[INFO] MLflow not available - skipping experiment setup. Clustering will work without tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Clustering Model (with Optional MLflow Tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns for clustering\n",
    "feature_cols = [\n",
    "    \"total_assessments\", \"avg_score\", \"avg_improvement\",\n",
    "    \"total_conversations\", \"avg_error_rate\", \"completed_lessons\", \"avg_mastery\"\n",
    "]\n",
    "\n",
    "# Build pipeline\n",
    "k_clusters = 4\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "kmeans = KMeans(k=k_clusters, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "# Train model (with optional MLflow tracking)\n",
    "if MLFLOW_AVAILABLE:\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=\"student_clustering_v1\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"k_clusters\", k_clusters)\n",
    "        mlflow.log_param(\"features\", feature_cols)\n",
    "        mlflow.log_param(\"n_users\", user_features.count())\n",
    "\n",
    "        # Fit model\n",
    "        model = pipeline.fit(user_features)\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = model.transform(user_features)\n",
    "\n",
    "        # Calculate cluster metrics\n",
    "        cluster_stats = predictions.groupBy(\"cluster\").agg(\n",
    "            F.count(\"*\").alias(\"user_count\"),\n",
    "            F.round(F.avg(\"avg_score\"), 1).alias(\"avg_score\"),\n",
    "            F.round(F.avg(\"total_conversations\"), 1).alias(\"avg_conversations\"),\n",
    "            F.round(F.avg(\"completed_lessons\"), 1).alias(\"avg_completed_lessons\")\n",
    "        ).orderBy(\"cluster\")\n",
    "\n",
    "        # Log metrics\n",
    "        kmeans_model = model.stages[-1]\n",
    "        mlflow.log_metric(\"inertia\", kmeans_model.summary.trainingCost)\n",
    "\n",
    "        for row in cluster_stats.collect():\n",
    "            mlflow.log_metric(f\"cluster_{row['cluster']}_size\", row[\"user_count\"])\n",
    "            mlflow.log_metric(f\"cluster_{row['cluster']}_avg_score\", row[\"avg_score\"])\n",
    "\n",
    "        # Log model\n",
    "        mlflow.spark.log_model(model, \"student_clustering_model\")\n",
    "\n",
    "        print(\"Model training complete with MLflow tracking!\")\n",
    "        print(f\"Inertia (within-cluster sum of squares): {kmeans_model.summary.trainingCost:.2f}\")\n",
    "else:\n",
    "    # Fit model without MLflow\n",
    "    model = pipeline.fit(user_features)\n",
    "    predictions = model.transform(user_features)\n",
    "    \n",
    "    # Calculate cluster metrics\n",
    "    cluster_stats = predictions.groupBy(\"cluster\").agg(\n",
    "        F.count(\"*\").alias(\"user_count\"),\n",
    "        F.round(F.avg(\"avg_score\"), 1).alias(\"avg_score\"),\n",
    "        F.round(F.avg(\"total_conversations\"), 1).alias(\"avg_conversations\"),\n",
    "        F.round(F.avg(\"completed_lessons\"), 1).alias(\"avg_completed_lessons\")\n",
    "    ).orderBy(\"cluster\")\n",
    "    \n",
    "    kmeans_model = model.stages[-1]\n",
    "    print(\"Model training complete (without MLflow tracking)!\")\n",
    "    print(f\"Inertia (within-cluster sum of squares): {kmeans_model.summary.trainingCost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cluster statistics\n",
    "cluster_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_analysis = predictions.groupBy(\"cluster\").agg(\n",
    "    F.count(\"*\").alias(\"users\"),\n",
    "    F.round(F.avg(\"avg_score\"), 1).alias(\"avg_assessment_score\"),\n",
    "    F.round(F.avg(\"avg_error_rate\"), 3).alias(\"avg_error_rate\"),\n",
    "    F.round(F.avg(\"completed_lessons\"), 1).alias(\"avg_lessons_completed\"),\n",
    "    F.round(F.avg(\"avg_mastery\"), 3).alias(\"avg_mastery_score\")\n",
    ").orderBy(\"cluster\")\n",
    "\n",
    "display(cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Clusters\n",
    "\n",
    "Based on the cluster characteristics, we can label them:\n",
    "- **Cluster 0**: High performers (high scores, many completed lessons)\n",
    "- **Cluster 1**: Active learners (many conversations, moderate scores)\n",
    "- **Cluster 2**: Struggling students (high error rates, lower scores)\n",
    "- **Cluster 3**: Casual users (low engagement, few assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster assignments to gold layer\n",
    "cluster_assignments = predictions.select(\"user_id\", \"cluster\")\n",
    "cluster_assignments.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_DB}.user_clusters\")\n",
    "\n",
    "print(f\"Saved {cluster_assignments.count()} user cluster assignments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gold Layer Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Summary of gold layer tables\n",
    "SELECT 'daily_metrics' as table_name, COUNT(*) as row_count FROM babblr_gold.daily_metrics\n",
    "UNION ALL\n",
    "SELECT 'lesson_effectiveness', COUNT(*) FROM babblr_gold.lesson_effectiveness\n",
    "UNION ALL\n",
    "SELECT 'cefr_funnel', COUNT(*) FROM babblr_gold.cefr_funnel\n",
    "UNION ALL\n",
    "SELECT 'topic_engagement', COUNT(*) FROM babblr_gold.topic_engagement\n",
    "UNION ALL\n",
    "SELECT 'user_clusters', COUNT(*) FROM babblr_gold.user_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Created daily metrics aggregates\n",
    "2. Calculated lesson effectiveness scores\n",
    "3. Built CEFR progression funnel\n",
    "4. Analyzed topic engagement\n",
    "5. **Trained K-Means clustering** (with optional MLflow tracking in paid editions)\n",
    "\n",
    "**Note:** MLflow is not available in Free Edition. The clustering model works without MLflow tracking. In paid editions, MLflow provides:\n",
    "- Parameter logging\n",
    "- Metric tracking\n",
    "- Model versioning\n",
    "- Reproducible experiments\n",
    "\n",
    "**Next:** Run `04_dashboard` for visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
