# ============================================================================
# Babblr Environment Configuration
# ============================================================================
# For detailed documentation on all settings, see: ../ENVIRONMENT.md
#
# Quick Start:
# 1. Copy this file: cp .env.example .env (or copy .env.example .env on Windows)
# 2. Configure your LLM provider (default: Ollama)
# 3. Restart the backend
# ============================================================================

# ============================================================================
# LLM PROVIDER SELECTION
# ============================================================================
# Choose which LLM provider to use: ollama (default), claude, gemini, mock
LLM_PROVIDER=ollama

# Common LLM settings (apply to all providers)
# LLM_MAX_TOKENS=1000
# LLM_TEMPERATURE=0.7
# LLM_TIMEOUT=60

# Retry settings for rate limits
# LLM_RETRY_ATTEMPTS=3
# LLM_RETRY_BASE_DELAY=1.0
# LLM_RETRY_MAX_DELAY=30.0

# ============================================================================
# OLLAMA SETTINGS (default provider - runs locally)
# ============================================================================
# Ollama must be running: ollama serve
# Pull a model first: ollama pull llama3.2:latest
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2:latest

# ============================================================================
# ANTHROPIC SETTINGS (Claude)
# ============================================================================
# Required if LLM_PROVIDER=claude
# Get your API key: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Model selection (optional, defaults to claude-sonnet-4-20250514)
# Options: claude-sonnet-4-20250514, claude-3-5-sonnet-20241022, claude-3-haiku-20240307
# See: https://docs.anthropic.com/en/docs/models-overview
# ANTHROPIC_MODEL=claude-sonnet-4-20250514

# ============================================================================
# GOOGLE GEMINI SETTINGS
# ============================================================================
# Required if LLM_PROVIDER=gemini
# GOOGLE_API_KEY=your_google_api_key_here
# GEMINI_MODEL=gemini-pro

# ============================================================================
# WHISPER SETTINGS (Speech-to-Text)
# ============================================================================
# Model size: tiny, base (default), small, medium, large
# Larger models are more accurate but slower and use more memory
# WHISPER_MODEL=base

# Device: auto (default), cuda (force GPU), cpu (force CPU)
# WHISPER_DEVICE=auto

# ============================================================================
# BABBLR API SERVER
# ============================================================================
BABBLR_API_HOST=127.0.0.1
BABBLR_API_PORT=8000

# ============================================================================
# BABBLR DATABASE
# ============================================================================
# SQLite database for conversations and vocabulary
# Future: May have separate databases for different purposes
BABBLR_CONVERSATION_DATABASE_URL=sqlite+aiosqlite:///./babblr.db

# ============================================================================
# BABBLR APPLICATION SETTINGS
# ============================================================================
# Frontend URL for CORS
BABBLR_FRONTEND_URL=http://localhost:3000

# Default timezone for timestamps (user's timezone from UI takes precedence)
# See: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
BABBLR_TIMEZONE=Europe/Amsterdam

# Development mode: saves audio files locally for debugging
# WARNING: Only enable for development, not production
# BABBLR_DEV_MODE=false

# Directory for audio files in development mode
# BABBLR_AUDIO_STORAGE_PATH=./audio_files
